
\chapter{Tensorgrad}

Implementation details

\section{Isomorphisms}
There is actually a concept of ``tensor isomorphism'', but it's basically just the same as graph isomorphism.

We need to understand isomorphisms in many different parts of the code.

\subsection{In Products}
- Cancelling / combining equal parts of a product
This is actually extra hard, because you have to collect a subset of nodes that constitute isomorphic subgraphs.
Right now we hack this a bit by just considering separate components of the product.

\begin{figure}[h]
\centering{
\def\svgwidth{\linewidth}
\import{figures/}{power.pdf_tex}
\caption{Combining equal parts of a product}
\label{fig:power}
}
\end{figure}

Basically the problem is:
\begin{enumerate}
   \item You are given a multigraph $G$ with nodes $V$ and edges $E$.
   \item Nodes and edges are all labeled.
   \item You are to find two disjoint subsets $V_1$ and $V_2$ of $V$ such that the subgraphs $G_1$ and $G_2$ induced by $V_1$ and $V_2$ are isomorphic.
      Also, under the isomorphism, the labels of the nodes and edges in $G_1$ and $G_2$ are the same.
\end{enumerate}

The problem is probably NP-hard, but it might still have an algorithm that's faster than
$2^n$ trying all subsets.
In particular, we might modify the VF2 algorithm, which iteratively tries to match nodes in $G_1$ and $G_2$.
The NetworkX library already has a GraphMatcher, which searches for isomoprhic subgraphs.
It might be extendable to our problem...
But honestly I don't know if we even want to solve this problem in the most general, since it corresponds a bit to factoring the graph.
And we don't do factoring, just as we don't do inverse distribution.

In either case, it's clear that we need to be able to compare nodes and edges for isomorphism.

Also, the basic usecase of isomorphism canonaization in products is simply to compute the canonical product itself from its parts.
Part of our approach here is taking the outer edges and turning them into nodes, so they can be colored.

\subsection{In Sums}

When deciding whether $A + B$ is equal to $2A$ we need to check if $A$ and $B$ are isomorphic.
But we also need to do this under the current renaming of the edges.
That's why you can't just transform $A + A^T = 2A$.

The way it actually works in my code is
\begin{lstlisting}
def key_fn(t: Tensor):
    # Align tensor edges to have the same order, using Sum's order as reference.
    canons = [t.canonical_edge_names[t.edges.index(e)] for e in self.edges]
    return hash((t.canon,) + tuple(canons))

ws_tensors = TensorDict(key_fn=key_fn, default_fn=int)
for w, t in zip(weights, tensors):
    ws_tensors[t] += w
ws_tensors = [(w, t) for t, w in ws_tensors.items()]
\end{lstlisting}
which says that I'm using for a hash, the canonical form of the tensor, plus the canonical form of the edges in the order of the edges in the sum.
These are basically the orbits, meaning that if the summed tensor has a symmetry, we are allowed to "flip" it to make the summands isomorphic.

In the ``compute canonical'' method, we do more or less the same, but we also include the weights.
\begin{lstlisting}
def _compute_canonical(self):
    hashes = []
    for e in self.edges:
        canons = [t.canonical_edge_names[t.edges.index(e)] for t in self.tensors]
        hashes.append(hash(("Sum",) + tuple(sorted(zip(self.weights, canons)))))
    base = hash(("Sum", len(self.tensors)))
    hashes = [hash((base, h)) for h in hashes]
    return base, hashes
\end{lstlisting}

In the future we want to use symmetry groups instead.
What would be the symmetry group of a sum?
It's the diagonal of the product of the symmetry groups of the summands.
How can we find the generators of this group?
Maybe we should just construct some joint graph and then find the automorphisms of that graph.

Alternatively we can use sympy.
It is not known whether this problem is solvable in polynomial time. I think Babai proved that it is quasi-polynomial but not with a practical algorithm. Incidentally the problems of intersections of subgroups, centralizers of elements, and stabilizers of subsets of $\{1, \dots, n\}$ have been proved (by Eugene Luks) to be polynomially equivalent.


Actually making a graph and using nauty is a really good idea, since it would
be able to detect that $A+A^T$ is symmetric.
Just taking the intersection of the automorphism groups of the summands would not find that.

Another option is to convert the sum to a function...
But no, that's weird.
That would require me to support functions with arbitrary numbers of inputs, which is not currently the case.



\subsection{In Evaluation}
When evaluating a tensor, we can look at the graph of the tensor and see if it's isomorphic to a previously evaluated tensor.
This is an example where we don't really need a canonical form, but an approximate hash plus vf2 would be fine.
Also note that in this case we don't care about the edge renaming, because we can just rename the edges before we return the tensor.
E.g. if we have already evaluated $A$, we can use that to get $A^T$ easily.

\subsection{In Variables}
In variables we include the name of the variable in the hash.
Basically we assume that variables named the same refer to the same data.
\begin{lstlisting}
   base = hash(("Variable", self.name))
   return base, [hash((base, e)) for e in self.original_edges]
\end{lstlisting}
For the original canonical edge names, we use the edge names before renaming.
This means, in the case of $A^T$ that will have the same hash as $A$.
But because it's renamed, the \texttt{t.index} call in the Sum will flip the edges.

We could imagine variables taking an automorphism group as an argument, which would allow us to define variables with different symmetries.
Such as a symmetric matrix $A$ where $A+A^T$ is actually $2A$.

\subsection{In Constants}
When computing the canonical form of a constant, like \texttt{Zero} or \texttt{Copy} we don't care about the edge names.
I guess because the constants we use are all maximally symmetric?
We currently include the constants \texttt{tag}, which is the hash of the variable that it came from, if any.

\subsection{In Functions}
One issue is that while the original names are usually part of the function definition,
the new edges added by differentiation are often automatically generated based on the context,
so they shouldn't really be part of the canonical form.

In contrast to Sum, we don't sort the canons here, since the order of the inputs matters.

Maybe functions should be allowed to transform the symmetry group?
E.g. if we have a function that takes a symmetric matrix and returns a symmetric matrix, we should be able to use the symmetry group of the input to simplify the output.

\subsection{In Derivatives}
All we do is hashing the tensor and the wrt.
And then add new edges for the derivative.

\subsection{Other}
For some tensors there might be edge dimension relations that aren't equivalences.
For example, a flatten tensor would have the ``out'' edge dimension equal to the product of the ``in'' edge dimensions.

In a previous version I had every tensor register a ``callback'' function.
Whenever an edge dimension ``became available'', the tensor would get a chance to emit new edge dimensions.
However, this was a lot more work for each tensor to implement, and not needed for any of the existing tensors.

\section{Renaming}
This is an important part of the code.

\section{Evaluation}

An important part of evaluation is determining the dimension of each edge.
To do this, I'm basically creating a full graph of the tensor, using a function called
\texttt{edge\_equivalences} which a list of tuples $((t_1, e_1), (t_2, e_2))$,
indicating that edge $e_1$ of tensor $t_1$ is equivalent to edge $e_2$ of tensor $t_2$.
Note that the same edge name can appear multiple times in the graph, so we need to keep track of the tensor as well.

For variables, since the user gives edge dimensions in terms of variables, it's important to keep track of renamed edge names:
\begin{lstlisting}
for e1, e2 in zip(self.original_edges, self.edges):
     yield (self, e1), (self, e2)
\end{lstlisting}

For constants, there might be some equivalences based on tensors that the constant was derived from.
\begin{lstlisting}
def edge_equivalences(self):
    if self.link is not None:
        yield from self.link.edge_equivalences()
        for e in self.link.edges:
            if e in self.edges:
                yield (self, e), (self.link, e)
\end{lstlisting}

For the copy tensor, everything is equivalent:
\begin{lstlisting}
def edge_equivalences(self):
    yield from super().edge_equivalences()
    for e in self.edges[1:]:
        yield (self, self.edges[0]), (self, e)
\end{lstlisting}

For functions we can't really say anything about the edges of the function itself \texttt{(self.edges\_out)},
but at least we can say something about the broadcasted edges.
\begin{lstlisting}
for t, *inner_edges in self.inputs:
    yield from t.edge_equivalences()
    for e in t.edges:
        if e not in inner_edges:
            yield (t, e), (self, e)
\end{lstlisting}
We could maybe also say that input edges with the same name are equivalent?

For products, we look at each edge $(t_1, e, t_2)$ and yield $(t_1, e), (t_2, e)$.
However for the free edges, $(t, e)$, we match them with ourselves, $(t, e), (self, e)$.
\begin{lstlisting}
def edge_equivalences(self):
    pairs = defaultdict(list)
    for t in self.tensors:
        yield from t.edge_equivalences()
        for e in t.edges:
            pairs[e].append(t)
    for e, ts in pairs.items():
        if len(ts) == 1:
            yield (self, e), (ts[0], e)
        else:
            t1, t2 = ts
            yield (t1, e), (t2, e)
\end{lstlisting}

Similarly, for sums, everything is just matched with ourselves:
\begin{lstlisting}
def edge_equivalences(self):
    for t in self.tensors:
        yield from t.edge_equivalences()
        for e in t.edges:
            yield (t, e), (self, e)
\end{lstlisting}

Finally, we use BFS to propagate the edge dimensions from the variables (which are given by the user) to the rest of the graph.

Why is it even necessary for non-variables to know the edge dimensions?
Mostly because of copy tensors, which we use for hyper edges, and have to construct.
Could we get rid of this if we computed hyper-edges more efficiently without copy's?
There are also sometimes "detached" copies...

Also, an alternative idea would be to actually construct the full graph.
I originally didn't think this would be possible because of the Sum's which aren't really graphs.
But maybe with the new approach of using nauty, we could actually do this.

\subsection{Products}
We simply evaluate the tensors in the product and give them to einsum.


\section{Simplification Rules}
There are a bunch of these.

Mostly we can do everything in a single depth-first pass,
but a few times we need to do multiple passes.
That can be done with the full-simplify method, which repeatedly calls simplify until nothing changes.
